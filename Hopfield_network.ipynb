{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hopfield-network.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPQ1zR1v0pkygL+xQdv/gtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalewkoz/neural-bifurcations/blob/main/Hopfield_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esPkD_MSYexT"
      },
      "source": [
        "# Noiseless ($T=0$) Hopfield network\n",
        "\n",
        "Let's simulate the Hopfield network with random patterns. The control parameter we focus here on is the ratio of the number of stored patterns to the number of neurons $\\alpha = P/N$. According to the mean-field theory, there is a phase transition at $\\alpha_c \\approx 0.14$. Let's check if we can observe it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "UKml_eooWNBt"
      },
      "source": [
        "# @title \n",
        "# @markdown Run this cell to import packages and load helper functions\n",
        "\n",
        "# Standard imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "from matplotlib import rc\n",
        "rc('font',**{'size':20})\n",
        "\n",
        "precision = torch.double\n",
        "\n",
        "## Check if a GPU available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU available: \"+torch.cuda.get_device_name())\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    print(\"No GPU available... Using CPU.\")\n",
        "    dev = \"cpu:0\"\n",
        "\n",
        "device = torch.device(dev)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def draw_evolution(X, mall, Nshow = -1, Tshow = -1, Pshow = -1):\n",
        "#fig = plt.figure(figsize=(20,8))\n",
        "    fig, ax = plt.subplots(1,2, figsize=(18,8))\n",
        "    \n",
        "    Z = X[0:Nshow,0:Tshow]\n",
        "    ax[0].imshow(Z)\n",
        "    ax[0].set_xlabel('Time')\n",
        "\n",
        "    mall = mall[:Pshow,:Tshow]\n",
        "    ax[1].plot(range(mall.size()[1]),mall.T)\n",
        "    ax[1].set_xlabel('Time')\n",
        "    ax[1].set_ylabel('m')\n",
        "\n",
        "def generate_patterns(N, P):\n",
        "    # Generate P random patterns of size N\n",
        "    patterns = 2*torch.randint(2,(N,P),device=device).type(precision)-1\n",
        "    return patterns\n",
        "\n",
        "def generate_J_mask(patterns, mask):\n",
        "    print(\"Generate J... \", end='')\n",
        "    N       = patterns.size()[0]\n",
        "    Jfull   = torch.matmul(patterns, patterns.T)\n",
        "    J       = Jfull*mask\n",
        "\n",
        "    Jdiag   = J.diagonal()\n",
        "    Jdiag   *= 0\n",
        "    print(\"Done!\")\n",
        "    return J\n",
        "\n",
        "def generate_J(patterns):\n",
        "    print(\"Generate J... \", end='')\n",
        "    N       = patterns.size()[0]\n",
        "    J       = torch.matmul(patterns, patterns.T) \n",
        "\n",
        "    Jdiag   = J.diagonal()\n",
        "    Jdiag   *= 0\n",
        "    print(\"Done!\")\n",
        "    return J\n",
        "\n",
        "def evolve_nn(J, x0, Nepoch=10):  \n",
        "    print(\"Simulate... \", end='')\n",
        "    N       = J.size()[0]\n",
        "    X = torch.zeros( (N, Nepoch+1), device=device, dtype=precision )\n",
        "    X[:,0] = x0\n",
        "    for i in range(Nepoch):\n",
        "        # Asynchronous update (much slower here, but should be closer the theoretical predictions)\n",
        "        X[:,i+1] = X[:,i]\n",
        "        for j in range(N):\n",
        "            X[j,i+1] =  2*(torch.matmul(J[j,:], X[:,i+1]) > 0 ) - 1\n",
        "        # Synchronous update:\n",
        "        #X[:,i+1] =  2*(torch.matmul(J, X[:,i]) > 0 ) - 1\n",
        "    print(\"Done!\")\n",
        "    return X\n",
        "\n",
        "def calc_corrs(patterns, ind=0):\n",
        "    return torch.matmul(patterns.T, patterns[:,0])/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97a9n0UZkKj"
      },
      "source": [
        "## Parameters\n",
        "N       = int(1e3)\n",
        "Nepoch  = int(1e2)\n",
        "alpha   = ...\n",
        "\n",
        "initErr = 0.1\n",
        "\n",
        "Pshow   = 10\n",
        "\n",
        "start = time.time()\n",
        "## Generate patterns and weights\n",
        "P           = int(alpha*N)\n",
        "patterns    = generate_patterns(N,P)\n",
        "J           = generate_J(patterns[:,:P])\n",
        "\n",
        "x0 = torch.clone(patterns[:,0])                 # Initial condition at 0th pattern...\n",
        "x0[0:int(initErr*N)] = -x0[0:int(initErr*N)]    # ...but we introduce some error (flip some bits)\n",
        "\n",
        "## Simulate\n",
        "X    = evolve_nn(J, x0 , Nepoch=Nepoch)\n",
        "\n",
        "## Analyze\n",
        "mall = torch.matmul(patterns[:,:Pshow].T, X)/N\n",
        "X    = X.cpu()#.numpy()\n",
        "mall = mall.cpu()\n",
        "stop = time.time()\n",
        "\n",
        "print(f\"Time on {dev} (torch): \"+str(stop-start))\n",
        "\n",
        "draw_evolution(X, mall, Nshow=Nepoch+1, Pshow=Pshow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huHmCRrBjjDb"
      },
      "source": [
        "Modify the code to incorporate diluted asymmetric connections. Note that the code to generate diluted synaptic weights is provided in the function _generate_J_mask_. You still have to generate the mask, which should contain many 0's (with probability $1 - K/N$) and some 1's (with probability $K/N$). $K$ should be much smaller than $N$, and in this case we redefine $\\alpha$ as $\\alpha = P/K$.\n",
        "\n",
        "How is the behavior of the system different in this case?\n",
        "\n",
        "See the original paper for details:\n",
        "\n",
        "_[Derrida, B., Gardner, E., & Zippelius, A. (1987). An exactly solvable asymmetric neural network model. EPL (Europhysics Letters), 4(2), 167.\n",
        "](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.1431&rep=rep1&type=pdf)_"
      ]
    }
  ]
}